{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to generate label for image location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current label structure\n",
    "1. places365 label\n",
    "   - 365 classes\n",
    "   - tag: indoor / outdoor / shop / exterior / interior / etc ...\n",
    "2. wordnet label\n",
    "   - find similar wordnet label\n",
    "3. SUN397 label\n",
    "   - 397 classes\n",
    "   - tag: indoor / outdoor / shop / exterior / interior / etc ...\n",
    "4. ADE20k (Option)\n",
    "   - a lot of object classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load places365 labels\n",
    "\n",
    "TODO: add indoor + outdoor label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labels: 365\n",
      "Labels saved to ../label_data/components/place365.txt\n"
     ]
    }
   ],
   "source": [
    "# Read and process labels from places365\n",
    "with open('../label_data/IO_places365.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "initial_label = []\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.split()[0].split('/')[2:] \n",
    "    \n",
    "    if len(parts) == 1:\n",
    "        # Format: /first-alphabet/label (without tag)\n",
    "        label = parts[0].replace('_', ' ')\n",
    "        initial_label.append(label)\n",
    "    elif len(parts) == 2:\n",
    "        # Format: /first-alphabet/label/tag\n",
    "        tag = parts[1].replace('_', ' ')\n",
    "        label = parts[0].replace('_', ' ')\n",
    "        combined_label = f\"{tag} {label}\"\n",
    "        initial_label.append(combined_label)\n",
    "\n",
    "print(\"Total labels:\", len(initial_label))    \n",
    "\n",
    "output_file_path = '../label_data/components/place365.txt'\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for label in initial_label:\n",
    "        output_file.write(label + '\\n')\n",
    "\n",
    "print(f\"Labels saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SUN397 label from tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload the SUN397 dataset\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "custom_data_dir = 'D:\\\\Code Space\\\\AI\\\\image_classification\\\\image\\\\data'\n",
    "\n",
    "dataset, info = tfds.load('sun397', split='train', with_info=True, data_dir=custom_data_dir)\n",
    "\n",
    "print(info)\n",
    "\n",
    "for sample in dataset.take(5):\n",
    "    image, label = sample['image'], sample['label']\n",
    "    print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labels: 397\n",
      "Labels saved to ../label_data/components/sun397.txt\n"
     ]
    }
   ],
   "source": [
    "# Read and process labels from places365\n",
    "with open('../label_data/SUN397_label.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "initial_label = []\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.strip().split('/')[2:]\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        # Format: /a/label\n",
    "        label = parts[0].replace('_', ' ')\n",
    "        initial_label.append(label)\n",
    "    elif len(parts) == 2:\n",
    "        # Format: /a/label/tag\n",
    "        tag = parts[1].replace('_', ' ')\n",
    "        label = parts[0].replace('_', ' ')\n",
    "        combined_label = f\"{tag} {label}\"\n",
    "        initial_label.append(combined_label)\n",
    "\n",
    "output_file_path = '../label_data/components/sun397.txt'\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for label in initial_label:\n",
    "        output_file.write(label + '\\n')\n",
    "\n",
    "print(f\"Total labels: {len(initial_label)}\")\n",
    "print(f\"Labels saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SUN397 + place365 label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique labels: 468\n",
      "Combined labels saved to ../label_data/components/place365_sun397.txt\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "sun397_file_path = '../label_data/components/sun397.txt'\n",
    "place365_file_path = '../label_data/components/place365.txt'\n",
    "output_file_path = '../label_data/components/place365_sun397.txt'\n",
    "\n",
    "# Initialize a set to hold all unique labels\n",
    "combined_labels = set()\n",
    "\n",
    "# Read labels from sun397.txt\n",
    "with open(sun397_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        label = line.strip()  # Remove any leading/trailing whitespace\n",
    "        combined_labels.add(label)\n",
    "\n",
    "# Read labels from place365.txt\n",
    "with open(place365_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        label = line.strip()  # Remove any leading/trailing whitespace\n",
    "        combined_labels.add(label)\n",
    "\n",
    "# Save the combined labels to place365_sun397.txt\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for label in sorted(combined_labels):  # Sort the labels for better readability\n",
    "        output_file.write(label + '\\n')\n",
    "\n",
    "print(f\"Total unique labels: {len(combined_labels)}\")\n",
    "print(f\"Combined labels saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find similar label with wordnet from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labels: 484\n",
      "Labels saved to ../label_data/components/place365_with_wordnet.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def is_location_related(synset):\n",
    "    \"\"\"\n",
    "    Check if a synset is related to locations.\n",
    "    It checks hypernyms for semantic similarity with common location synsets.\n",
    "    \"\"\"\n",
    "    location_hypernyms = {\n",
    "        'place.n.02', 'region.n.01', 'geographic_area.n.01', 'facility.n.01',\n",
    "        'building.n.01', 'natural_object.n.01', 'residential_area.n.01',\n",
    "        'body_of_water.n.01', 'recreational_area.n.01', 'urban_area.n.01',\n",
    "        'rural_area.n.01', 'land.n.02', 'infrastructure.n.01'\n",
    "    }\n",
    "\n",
    "    for hypernym in synset.hypernyms():\n",
    "        if hypernym.name() in location_hypernyms:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_location_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        if syn.pos() == 'n' and is_location_related(syn):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return synonyms\n",
    "\n",
    "for label in initial_label[:]:  \n",
    "    synonyms = get_location_synonyms(label)\n",
    "    initial_label.extend(synonyms - set(initial_label))\n",
    "    \n",
    "print(\"Total labels:\", len(initial_label))\n",
    "    \n",
    "output_file_path = '../label_data/components/place365_with_wordnet.txt'\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for label in initial_label:\n",
    "        output_file.write(label + '\\n')\n",
    "\n",
    "print(f\"Labels saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words related to specified location hypernyms:\n",
      "Eden\n",
      "Edgeworth-Kuiper belt\n",
      "Hall of Fame\n",
      "Kuiper belt\n",
      "Papua\n",
      "Roman building\n",
      "Shangri-la\n",
      "Sind\n",
      "abattoir\n",
      "aerospace\n",
      "air\n",
      "airfield\n",
      "apartment building\n",
      "apartment house\n",
      "arboretum\n",
      "architecture\n",
      "assembly\n",
      "asterism\n",
      "athletic facility\n",
      "atmosphere\n",
      "aviary\n",
      "backroom\n",
      "backwater\n",
      "badlands\n",
      "bagnio\n",
      "barrio\n",
      "bathhouse\n",
      "bathing machine\n",
      "bawdyhouse\n",
      "bay\n",
      "belt\n",
      "biosphere\n",
      "bird sanctuary\n",
      "black body\n",
      "black hole\n",
      "blackbody\n",
      "boatyard\n",
      "body\n",
      "bordello\n",
      "botanical garden\n",
      "bottom\n",
      "bottomland\n",
      "bowling alley\n",
      "briny\n",
      "brothel\n",
      "butchery\n",
      "cafeteria facility\n",
      "carpet\n",
      "casino-hotel\n",
      "cathouse\n",
      "celestial body\n",
      "center\n",
      "centre\n",
      "channel\n",
      "chapterhouse\n",
      "club\n",
      "clubhouse\n",
      "coastland\n",
      "cocoon\n",
      "colony\n",
      "communication equipment\n",
      "communication system\n",
      "consolidation\n",
      "constellation\n",
      "conurbation\n",
      "cosmos\n",
      "county\n",
      "course\n",
      "cover\n",
      "covering\n",
      "creation\n",
      "crossing\n",
      "cultivated land\n",
      "dead body\n",
      "dead room\n",
      "deep space\n",
      "deposit\n",
      "depositary\n",
      "depository\n",
      "depth\n",
      "distance\n",
      "dorm\n",
      "dormitory\n",
      "drink\n",
      "drive-in\n",
      "eatery\n",
      "eating house\n",
      "eating place\n",
      "embayment\n",
      "estraterrestrial body\n",
      "estuary\n",
      "existence\n",
      "exterior\n",
      "extraterrestrial object\n",
      "extremity\n",
      "falls\n",
      "farm building\n",
      "farmland\n",
      "feedlot\n",
      "field\n",
      "firetrap\n",
      "flowage\n",
      "flying field\n",
      "ford\n",
      "forum\n",
      "full radiator\n",
      "gambling den\n",
      "gambling hell\n",
      "gambling house\n",
      "gaming house\n",
      "gas system\n",
      "gazebo\n",
      "glasshouse\n",
      "government building\n",
      "greenhouse\n",
      "greensward\n",
      "grid\n",
      "gulf\n",
      "hall\n",
      "hatchery\n",
      "health facility\n",
      "healthcare facility\n",
      "heaven\n",
      "heavenly body\n",
      "heliosphere\n",
      "hell\n",
      "hell on earth\n",
      "hellhole\n",
      "high sea\n",
      "hotel\n",
      "hotel-casino\n",
      "house\n",
      "house of God\n",
      "house of ill repute\n",
      "house of prayer\n",
      "house of prostitution\n",
      "house of worship\n",
      "inferno\n",
      "inlet\n",
      "inside\n",
      "intergalactic space\n",
      "interior\n",
      "international waters\n",
      "interplanetary space\n",
      "interstellar space\n",
      "ionosphere\n",
      "lake\n",
      "landing field\n",
      "layer\n",
      "library\n",
      "macrocosm\n",
      "main\n",
      "mansion\n",
      "mare\n",
      "maria\n",
      "mechanism\n",
      "medical building\n",
      "meeting place\n",
      "megalopolis\n",
      "menagerie\n",
      "mid-water\n",
      "military installation\n",
      "ministry\n",
      "morgue\n",
      "mortuary\n",
      "municipality\n",
      "natural covering\n",
      "nest\n",
      "new town\n",
      "nirvana\n",
      "nursery\n",
      "observatory\n",
      "ocean\n",
      "office block\n",
      "office building\n",
      "offing\n",
      "opium den\n",
      "organic structure\n",
      "outbuilding\n",
      "outside\n",
      "overburden\n",
      "packinghouse\n",
      "paradise\n",
      "permafrost\n",
      "physical structure\n",
      "place of worship\n",
      "planetarium\n",
      "planetary house\n",
      "plant part\n",
      "plant structure\n",
      "ploughland\n",
      "plowland\n",
      "polder\n",
      "polynya\n",
      "pool\n",
      "power grid\n",
      "power system\n",
      "presbytery\n",
      "promised land\n",
      "puddle\n",
      "radiator\n",
      "radius\n",
      "range\n",
      "rangeland\n",
      "recess\n",
      "recreation facility\n",
      "recreational facility\n",
      "repository\n",
      "residence hall\n",
      "rest house\n",
      "restaurant\n",
      "rink\n",
      "rock\n",
      "rotunda\n",
      "ruin\n",
      "sample\n",
      "sanctuary\n",
      "scablands\n",
      "school\n",
      "schoolhouse\n",
      "sea\n",
      "seven seas\n",
      "sewage system\n",
      "sewage works\n",
      "sewer system\n",
      "shallow\n",
      "shambles\n",
      "shoal\n",
      "shooting gallery\n",
      "side\n",
      "sign\n",
      "sign of the zodiac\n",
      "signal box\n",
      "signal tower\n",
      "skating rink\n",
      "skyscraper\n",
      "slaughterhouse\n",
      "snake pit\n",
      "sod\n",
      "sound\n",
      "source\n",
      "sporting house\n",
      "sprawl\n",
      "star sign\n",
      "station\n",
      "stone\n",
      "stream\n",
      "student residence\n",
      "student union\n",
      "summerhouse\n",
      "sward\n",
      "tangle\n",
      "tap house\n",
      "tavern\n",
      "telco building\n",
      "telecom hotel\n",
      "temple\n",
      "territorial waters\n",
      "the pits\n",
      "theater\n",
      "theatre\n",
      "tillage\n",
      "tilled land\n",
      "tilth\n",
      "top\n",
      "transit\n",
      "transportation\n",
      "transportation system\n",
      "turf\n",
      "universe\n",
      "urban sprawl\n",
      "utility\n",
      "vacuity\n",
      "vacuum\n",
      "volary\n",
      "water\n",
      "water supply\n",
      "water system\n",
      "watercourse\n",
      "waterfall\n",
      "waterway\n",
      "wetland\n",
      "whorehouse\n",
      "world\n",
      "zodiac\n",
      "zone\n",
      "zoo\n",
      "zoological garden\n"
     ]
    }
   ],
   "source": [
    "# Find words that are related to at least one of the specified location hypernyms\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "location_hypernyms = {\n",
    "    'place.n.02', 'region.n.01', 'geographic_area.n.01', 'facility.n.01',\n",
    "    'building.n.01', 'natural_object.n.01', 'residential_area.n.01',\n",
    "    'body_of_water.n.01', 'urban_area.n.01',\n",
    "    'rural_area.n.01', 'land.n.02', 'infrastructure.n.01'\n",
    "}\n",
    "\n",
    "\n",
    "location_words = set()\n",
    "\n",
    "for synset in wordnet.all_synsets(pos='n'):\n",
    "    if any(hypernym.name() in location_hypernyms for hypernym in synset.hypernyms()):\n",
    "        location_words.update([lemma.name().replace('_', ' ') for lemma in synset.lemmas()])\n",
    "\n",
    "print(\"Words related to specified location hypernyms:\")\n",
    "for word in sorted(location_words):\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ADE20k label from https://ade20k.csail.mit.edu/index.html#Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "label_folder = '../label_data/'\n",
    "json_file_path = os.path.join(label_folder, 'ADE20K_treeData.json')\n",
    "file_path = os.path.join(label_folder, 'ADE20K_label.txt')\n",
    "\n",
    "if os.path.exists(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "        tree_data = json.load(json_file)\n",
    "        print(\"Loaded treeData from existing JSON file:\")\n",
    "        print(json.dumps(tree_data, indent=2, ensure_ascii=False))\n",
    "\n",
    "else:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    tree_data_match = re.search(r'var treeData = (\\[.*\\]);', data, re.DOTALL)\n",
    "    tree_data_str = None\n",
    "\n",
    "    if tree_data_match:\n",
    "        tree_data_str = tree_data_match.group(1)\n",
    "\n",
    "    if tree_data_str:\n",
    "        tree_data_str = tree_data_str.replace(\"'\", '\"')\n",
    "        tree_data_str = re.sub(r',(\\s*[\\]}])', r'\\1', tree_data_str)\n",
    "\n",
    "        try:\n",
    "            tree_data = json.loads(tree_data_str)\n",
    "            with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(tree_data, json_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "            print(f\"treeData saved to {json_file_path}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing treeData: {e}\")\n",
    "    else:\n",
    "        print(\"treeData not found or the format is incorrect.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing all label from different dataset into file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize place365 + sun397 + wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique labels: 742\n",
      "Combined labels saved to ../label_data/final/final_label.txt\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "sun397_and_place365_file_path = '../label_data/components/place365_and_sun397_with_wordnet.txt'\n",
    "word_net_file_path = '../label_data/components/word_net.txt'\n",
    "output_file_path = '../label_data/final/final_label.txt'\n",
    "\n",
    "# Initialize a set to hold all unique labels\n",
    "combined_labels = set()\n",
    "\n",
    "# Read labels from sun397.txt\n",
    "with open(sun397_and_place365_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        label = line.strip()  # Remove any leading/trailing whitespace\n",
    "        combined_labels.add(label)\n",
    "\n",
    "# Read labels from place365.txt\n",
    "with open(word_net_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        label = line.strip()  # Remove any leading/trailing whitespace\n",
    "        combined_labels.add(label)\n",
    "\n",
    "# Save the combined labels to place365_sun397.txt\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for label in sorted(combined_labels):  # Sort the labels for better readability\n",
    "        output_file.write(label + '\\n')\n",
    "\n",
    "print(f\"Total unique labels: {len(combined_labels)}\")\n",
    "print(f\"Combined labels saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Add location from COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Context Descriptor from Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import time\n",
    "import json\n",
    "\n",
    "API_KEY = 'AIzaSyD52j5LGrj2siH0dgt-HvhzYRpmfpJhX4Q'\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "input_file_path = \"../label_data/final/final_label.txt\"\n",
    "existing_labels = []\n",
    "\n",
    "with open(input_file_path, 'r') as file:\n",
    "    existing_labels = [line.strip() for line in file]\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "batch_size = 50  # Adjust as needed based on token usage\n",
    "\n",
    "def generate_context_descriptors_batch(labels):\n",
    "    \"\"\"\n",
    "    Generates context descriptors for a batch of labels.\n",
    "    \"\"\"\n",
    "    labels_str = \" \".join([f\"'{label}'\" for label in labels])\n",
    "    prompt = (\n",
    "        f\"Generate 10 noun phrases that either contain or are related to each of the following labels. \"\n",
    "        f\"The phrases should be relevant for describing different scenes or locations. \"\n",
    "        f\"Separate the phrases by dashes, without any additional text or formatting. \"\n",
    "        f\"Format: word-nounphrase1-nounphrase2...\\\\word2-nounphrase1-...\\\\...: {labels_str}.\"\n",
    "        f\"Remember no formatting like bold or italic\"\n",
    "    )\n",
    "\n",
    "    chat = model.start_chat()\n",
    "\n",
    "    response = chat.send_message(prompt)\n",
    "    \n",
    "    print(response)\n",
    "\n",
    "    response_text = response.text.strip()\n",
    "    \n",
    "    print(response_text)\n",
    "\n",
    "    label_context_pairs = response_text.split('\\n')\n",
    "\n",
    "    context_descriptors = {}\n",
    "    for pair in label_context_pairs:\n",
    "        if '-' in pair:\n",
    "            parts = pair.split('-')\n",
    "            label = parts[0]\n",
    "            context_words = parts[1:]\n",
    "            context_descriptors[label] = context_words\n",
    "\n",
    "    return context_descriptors\n",
    "\n",
    "context_descriptors = {}\n",
    "\n",
    "for i in range(0, len(existing_labels), batch_size):\n",
    "    batch_labels = existing_labels[i:i + batch_size]\n",
    "    batch_context_descriptors = generate_context_descriptors_batch(batch_labels)\n",
    "\n",
    "    context_descriptors.update(batch_context_descriptors)\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "output_file_path = '../label_data/final/context_descriptors.json'\n",
    "\n",
    "with open(output_file_path, 'w') as json_file:\n",
    "    json.dump(context_descriptors, json_file, indent=2)\n",
    "\n",
    "print(f\"Context descriptors saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
