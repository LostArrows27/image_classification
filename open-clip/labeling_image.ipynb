{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from tkinter import filedialog\n",
    "import open_clip\n",
    "\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('convnext_base_w', pretrained='laion2b_s13b_b82k_augreg')\n",
    "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = filedialog.askopenfilename(title=\"Choose a image\", filetypes=[(\"Image files\", \"*.png *.jpg *.jpeg\")])\n",
    "\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "\n",
    "location_labels = [\"flower\", \"garden\", \"flower garden\", \"beach\", \"mountain\", \"city\", \"forest\", \"desert\", \"village\"]\n",
    "\n",
    "locationn_description = tokenizer([\"This is located at \" + location for location in location_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower garden: 71.4617%\n",
      "garden: 16.1532%\n",
      "flower: 12.2007%\n",
      "village: 0.1202%\n",
      "forest: 0.0400%\n",
      "mountain: 0.0121%\n",
      "city: 0.0121%\n",
      "beach: 0.0001%\n",
      "desert: 0.0000%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(locationn_description)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    location_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Sort the location descriptions by their probabilities in descending order\n",
    "sorted_probs_and_labels = sorted(zip(location_labels, location_probs[0]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted results\n",
    "for location, prob in sorted_probs_and_labels:\n",
    "    print(f\"{location}: {100*prob.item():.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
